# M31R Training Configuration
# See docs/07_TRAINING_ARCHITECTURE.md for the full spec.

train:
  config_version: "1.0.0"
  batch_size: 8
  gradient_accumulation_steps: 4
  max_steps: 100000
  learning_rate: 3e-4
  min_learning_rate: 1e-5
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  warmup_steps: 1000
  precision: "bf16"
  checkpoint_interval: 1000
  log_interval: 10
  dataset_directory: "data/datasets"
  tokenizer_directory: "data/tokenizer"
