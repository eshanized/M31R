# M31R Overfit Sanity Test â€” Unified Config
# Intentionally tiny model + high LR to force memorization of repeated data.
# If training cannot overfit this, the system is broken.
#
# CPU-only, deterministic, ~2 minutes, fp32.

global:
  config_version: "1.0.0"
  project_name: "m31r"
  seed: 42
  log_level: "INFO"
  directories:
    data: "data"
    checkpoints: "checkpoints"
    logs: "logs"
    experiments: "experiments"
    configs: "configs"

model:
  config_version: "1.0.0"
  n_layers: 2
  hidden_size: 64
  n_heads: 2
  head_dim: 32
  context_length: 128
  dropout: 0.0
  norm_eps: 1e-6
  rope_theta: 10000.0
  init_std: 0.02
  vocab_size: 256

train:
  config_version: "1.0.0"
  batch_size: 4
  gradient_accumulation_steps: 1
  max_steps: 500
  learning_rate: 1e-2
  min_learning_rate: 1e-4
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  grad_clip: 1.0
  warmup_steps: 5
  precision: "fp32"
  checkpoint_interval: 100
  log_interval: 10
  dataset_directory: "datasets/dev-overfit"
  tokenizer_directory: "data/tokenizer-overfit"

runtime:
  config_version: "1.0.0"
  device: "cpu"
  quantization: "none"
  max_tokens: 64
  temperature: 0.0
  top_k: 0
  max_context_length: 128
  stream: false
  host: "127.0.0.1"
  port: 8731
  max_request_size_bytes: 1048576
  request_timeout_seconds: 30
  model_path: "release/overfit"
  tokenizer_path: "data/tokenizer-overfit"
  seed: 42
